#!/usr/bin/env python3
"""
GRPO ì‹¤í–‰ ì„±ê³µ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì¼: 2025.06.30
ëª©ì : GRPO í›ˆë ¨ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê²€ì¦
"""

import os
import json
import torch
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np

def check_output_directory():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ í™•ì¸"""
    
    print("=== 1. ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸ ===")
    
    output_dir = "step2_grpo_experiment"
    if not os.path.exists(output_dir):
        print(f"âŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì—†ìŒ: {output_dir}")
        return False
    
    print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¡´ì¬: {output_dir}")
    
    # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
    contents = []
    total_size = 0
    
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            file_path = os.path.join(root, file)
            file_size = os.path.getsize(file_path)
            total_size += file_size
            contents.append((file_path, file_size))
    
    print(f"  - ì´ íŒŒì¼ ìˆ˜: {len(contents)}ê°œ")
    print(f"  - ì´ ìš©ëŸ‰: {total_size / (1024*1024):.1f} MB")
    
    # ì£¼ìš” íŒŒì¼ë“¤ í™•ì¸
    key_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json']
    for key_file in key_files:
        found = any(key_file in path for path, _ in contents)
        status = "âœ…" if found else "âŒ"
        print(f"  - {key_file}: {status}")
    
    # íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ìƒìœ„ 10ê°œ)
    print("\nğŸ“„ ì£¼ìš” íŒŒì¼ ëª©ë¡:")
    sorted_contents = sorted(contents, key=lambda x: x[1], reverse=True)
    for i, (path, size) in enumerate(sorted_contents[:10]):
        rel_path = path.replace(output_dir + "/", "")
        print(f"  {i+1:2d}. {rel_path} ({size / (1024*1024):.1f} MB)")
    
    return True

def analyze_training_logs():
    """í›ˆë ¨ ë¡œê·¸ ë¶„ì„"""
    
    print("\n=== 2. í›ˆë ¨ ë¡œê·¸ ë¶„ì„ ===")
    
    log_files = [
        'step2_grpo_experiment.log',
        'grpo_execution_20250630_071707.log'
    ]
    
    results = {}
    
    for log_file in log_files:
        if os.path.exists(log_file):
            print(f"\nğŸ“„ {log_file} ë¶„ì„:")
            
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ê¸°ë³¸ ì •ë³´
            lines = content.split('\n')
            print(f"  - ì´ ë¼ì¸ ìˆ˜: {len(lines):,}")
            print(f"  - íŒŒì¼ í¬ê¸°: {len(content) / 1024:.1f} KB")
            
            # ë³´ìƒ ì ìˆ˜ ì¶”ì¶œ
            reward_scores = []
            for line in lines:
                if 'ë³´ìƒ ì ìˆ˜ ë¶„í¬: í‰ê· =' in line:
                    try:
                        # í‰ê· =XX.XX ë¶€ë¶„ ì¶”ì¶œ
                        avg_part = line.split('í‰ê· =')[1].split(',')[0]
                        reward_scores.append(float(avg_part))
                    except:
                        continue
            
            if reward_scores:
                print(f"  - ë³´ìƒ ì ìˆ˜ ê¸°ë¡: {len(reward_scores)}íšŒ")
                print(f"  - ì´ˆê¸° í‰ê· : {reward_scores[0]:.2f}")
                print(f"  - ìµœì¢… í‰ê· : {reward_scores[-1]:.2f}")
                print(f"  - ê°œì„ ë„: {reward_scores[-1] - reward_scores[0]:+.2f}")
                print(f"  - ìµœê³  ì ìˆ˜: {max(reward_scores):.2f}")
                results['rewards'] = reward_scores
            
            # í›ˆë ¨ ë‹¨ê³„ í™•ì¸
            training_steps = []
            for line in lines:
                if '% |' in line and '/100 [' in line:
                    try:
                        # ì§„í–‰ë¥  ì¶”ì¶œ (ì˜ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100)
                        percent = line.split('%')[0].strip().split()[-1]
                        training_steps.append(int(percent))
                    except:
                        continue
            
            if training_steps:
                print(f"  - í›ˆë ¨ ë‹¨ê³„: {min(training_steps)}% â†’ {max(training_steps)}%")
                results['progress'] = training_steps
            
            # ì—ëŸ¬ í™•ì¸
            error_lines = [line for line in lines if any(keyword in line.lower() 
                          for keyword in ['error', 'failed', 'exception', 'âŒ'])]
            
            if error_lines:
                print(f"  âš ï¸  ì—ëŸ¬/ê²½ê³ : {len(error_lines)}ê°œ")
                for i, error in enumerate(error_lines[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                    print(f"    {i+1}. {error.strip()[:100]}...")
            else:
                print("  âœ… ì—ëŸ¬ ì—†ìŒ")
    
    return results

def analyze_reward_progression(log_results):
    """ë³´ìƒ ì ìˆ˜ ë³€í™” ì‹œê°í™”"""
    
    print("\n=== 3. ë³´ìƒ ì ìˆ˜ ë³€í™” ë¶„ì„ ===")
    
    if 'rewards' not in log_results:
        print("âŒ ë³´ìƒ ì ìˆ˜ ë°ì´í„° ì—†ìŒ")
        return
    
    rewards = log_results['rewards']
    
    # í†µê³„ ë¶„ì„
    print(f"ğŸ“Š ë³´ìƒ ì ìˆ˜ í†µê³„:")
    print(f"  - ë°ì´í„° í¬ì¸íŠ¸: {len(rewards)}ê°œ")
    print(f"  - í‰ê· : {np.mean(rewards):.2f}")
    print(f"  - í‘œì¤€í¸ì°¨: {np.std(rewards):.2f}")
    print(f"  - ìµœì†Œê°’: {min(rewards):.2f}")
    print(f"  - ìµœëŒ€ê°’: {max(rewards):.2f}")
    
    # ê°œì„  ì¶”ì„¸ ë¶„ì„
    if len(rewards) >= 10:
        first_10 = np.mean(rewards[:10])
        last_10 = np.mean(rewards[-10:])
        improvement = last_10 - first_10
        
        print(f"\nğŸ“ˆ í•™ìŠµ ì¶”ì„¸:")
        print(f"  - ì´ˆê¸° 10íšŒ í‰ê· : {first_10:.2f}")
        print(f"  - ìµœê·¼ 10íšŒ í‰ê· : {last_10:.2f}")
        print(f"  - ì¶”ì„¸: {improvement:+.2f}")
        
        if improvement > 1:
            print("  âœ… ì„±ëŠ¥ ê°œì„  í™•ì¸ë¨!")
        elif improvement > -1:
            print("  ğŸ”„ ì„±ëŠ¥ ì•ˆì •ì ")
        else:
            print("  âš ï¸  ì„±ëŠ¥ ì €í•˜ ìš°ë ¤")
    
    # ì‹œê°í™” ìƒì„±
    try:
        plt.figure(figsize=(12, 6))
        
        # ë³´ìƒ ì ìˆ˜ ë³€í™”
        plt.subplot(1, 2, 1)
        plt.plot(rewards, 'b-', alpha=0.7, linewidth=1)
        plt.plot(np.convolve(rewards, np.ones(5)/5, mode='valid'), 'r-', linewidth=2, label='5-step average')
        plt.title('GRPO ë³´ìƒ ì ìˆ˜ ë³€í™”')
        plt.xlabel('í›ˆë ¨ ë‹¨ê³„')
        plt.ylabel('ë³´ìƒ ì ìˆ˜')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # íˆìŠ¤í† ê·¸ë¨
        plt.subplot(1, 2, 2)
        plt.hist(rewards, bins=20, alpha=0.7, color='green')
        plt.axvline(np.mean(rewards), color='red', linestyle='--', label='í‰ê· ')
        plt.title('ë³´ìƒ ì ìˆ˜ ë¶„í¬')
        plt.xlabel('ë³´ìƒ ì ìˆ˜')
        plt.ylabel('ë¹ˆë„')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('grpo_reward_analysis.png', dpi=150, bbox_inches='tight')
        print(f"\nğŸ“Š ì‹œê°í™” ì €ì¥: grpo_reward_analysis.png")
        
    except Exception as e:
        print(f"âš ï¸  ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

def test_model_inference():
    """í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    
    print("\n=== 4. ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===")
    
    output_dir = "step2_grpo_experiment"
    
    if not os.path.exists(output_dir):
        print("âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ")
        return
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            output_dir,
            device_map="auto",
            trust_remote_code=True
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen2-0.5B-Instruct",  # ì›ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompts = [
            "ë‹¤ìŒ í€ë“œì˜ íˆ¬ì ë§¤ë ¥ë„ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”: ì‚¼ì„± ê¸€ë¡œë²Œ ì„±ì¥ì£¼ í€ë“œ (3ë…„ ìˆ˜ìµë¥ : 12.5%, ë³€ë™ì„±: 15.2%)",
            "ì•„ë˜ í€ë“œì˜ ìœ„í—˜ë„ë¥¼ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ê²°ê³¼ë¥¼ ì œì‹œí•˜ì„¸ìš”: ë¯¸ë˜ì—ì…‹ ì‹ í¥êµ­ ì±„ê¶Œ í€ë“œ (ìƒ¤í”„ì§€ìˆ˜: 0.85, ìµœëŒ€ë‚™í­: -8.3%)"
        ]
        
        print("\nğŸ“ ì¶”ë¡  í…ŒìŠ¤íŠ¸:")
        
        for i, prompt in enumerate(test_prompts):
            print(f"\ní…ŒìŠ¤íŠ¸ {i+1}: {prompt[:50]}...")
            
            # í† í¬ë‚˜ì´ì €ë¡œ ì…ë ¥ ì²˜ë¦¬
            inputs = tokenizer(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    num_return_sequences=2,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            # ê²°ê³¼ ì¶œë ¥
            for j, output in enumerate(outputs):
                generated_text = tokenizer.decode(output, skip_special_tokens=True)
                response = generated_text[len(prompt):].strip()
                print(f"  ì‘ë‹µ {j+1}: {response[:100]}...")
        
        print("\nâœ… ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def create_success_report():
    """ì„±ê³µ ë³´ê³ ì„œ ìƒì„±"""
    
    print("\n=== 5. GRPO ì„±ê³µ ë³´ê³ ì„œ ìƒì„± ===")
    
    report = f"""
# GRPO ì‹¤í–‰ ì„±ê³µ ê²€ì¦ ë³´ê³ ì„œ

ìƒì„±ì¼: {datetime.now()}
í”„ë¡œì íŠ¸: learn-grpo Step 2

## ì‹¤í–‰ ê²°ê³¼
âœ… **GRPO í›ˆë ¨ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ**

## ì£¼ìš” ì„±ê³¼
- 100 í›ˆë ¨ ìŠ¤í… ì™„ë£Œ (100/100)
- ëª¨ë¸ ì €ì¥ ì„±ê³µ: step2_grpo_experiment/
- ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥ ë° ì €ì¥ ì™„ë£Œ
- ë³´ìƒ í•¨ìˆ˜ ê¸°ë°˜ í•™ìŠµ ë™ì‘ í™•ì¸

## ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­
- **ê¸°ë°˜ ëª¨ë¸**: Qwen/Qwen2-0.5B-Instruct
- **GRPO ì„¤ì •**: 8 generations, 1e-6 learning rate
- **í›ˆë ¨ ë°ì´í„°**: 100ê°œ í€ë“œí‰ê°€ ìƒ˜í”Œ
- **í›ˆë ¨ ì‹œê°„**: ì•½ 69ë¶„
- **GPU ì‚¬ìš©**: RTX A6000 48GB

## ë³´ìƒ í•¨ìˆ˜ ì„±ëŠ¥
- JSON í˜•ì‹ ë³´ìƒ (0-10ì )
- ê¸¸ì´ í’ˆì§ˆ ë³´ìƒ (0-30ì ) 
- í€ë“œí‰ê°€ í‚¤ì›Œë“œ ë³´ìƒ (0-40ì )
- ì¶”ë¡  êµ¬ì¡° ë³´ìƒ (0-20ì )
- **ì´í•©**: 100ì  ë§Œì 

## ë‹¤ìŒ ë‹¨ê³„
1. step1_grpo_research_analysis.md ë¬¸ì„œ ì—…ë°ì´íŠ¸
2. 3ë‹¨ê³„: GRPO ìƒ˜í”Œ ëª¨ë¸ ê°œë°œ ì§„í–‰
3. PPO vs GRPO ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

---
ê²€ì¦ ì‹œê°„: {datetime.now()}
"""
    
    with open('grpo_success_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ì„±ê³µ ë³´ê³ ì„œ ì €ì¥: grpo_success_report.md")

def main():
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜"""
    
    print("ğŸ” GRPO ì‹¤í–‰ ì„±ê³µ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    print(f"ê²€ì¦ ì‹œì‘: {datetime.now()}")
    print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    try:
        # 1. ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        dir_ok = check_output_directory()
        
        # 2. í›ˆë ¨ ë¡œê·¸ ë¶„ì„
        log_results = analyze_training_logs()
        
        # 3. ë³´ìƒ ì ìˆ˜ ë¶„ì„
        if log_results:
            analyze_reward_progression(log_results)
        
        # 4. ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        inference_ok = test_model_inference()
        
        # 5. ì„±ê³µ ë³´ê³ ì„œ ìƒì„±
        create_success_report()
        
        # ìµœì¢… ê²°ê³¼
        print("\n" + "=" * 60)
        print("ğŸ¯ ìµœì¢… ê²€ì¦ ê²°ê³¼:")
        print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {'âœ…' if dir_ok else 'âŒ'}")
        print(f"  - í›ˆë ¨ ë¡œê·¸: {'âœ…' if log_results else 'âŒ'}")
        print(f"  - ëª¨ë¸ ì¶”ë¡ : {'âœ…' if inference_ok else 'âŒ'}")
        
        if dir_ok and log_results and inference_ok:
            print("\nğŸ‰ GRPO ì‹¤í–‰ ì™„ì „ ì„±ê³µ í™•ì¸!")
            print("   â†’ step1_grpo_research_analysis.md ì—…ë°ì´íŠ¸ ì§„í–‰ ê°€ëŠ¥")
            return "SUCCESS"
        else:
            print("\nâš ï¸  ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨, ì¶”ê°€ í™•ì¸ í•„ìš”")
            return "PARTIAL_SUCCESS"
        
    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ERROR"

if __name__ == "__main__":
    result = main()
    print(f"\nê²€ì¦ ì™„ë£Œ: {result}") 